---
title: 'Problem Set 3: Modeling'
author: "Campbell/Langasek"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Due Date: August 31 (Monday of Week 10)

1) Calculate the LACE scores for each patient. 

2 Run a basic analysis of the LACE variables to predict readmission risk using your analytic file. Some kinds of analysis you can do: contingency tables ([more info here](http://www.statmethods.net/stats/frequencies.html) ) or logistic regression (More info [here](https://www.r-bloggers.com/evaluating-logistic-regression-models/) and [here](https://www.analyticsvidhya.com/blog/2015/11/beginners-guide-on-logistic-regression-in-r/) ). How well does the LACE score predict readmission risk? Can you compare the predictive power of each variable (L, A, C, E) to predict risk? Begin to think about what visualizations and analysis supports your conclusions.

3) *Optional*: Try changing the ‘weights’ of each of the individual scores to calculate a different LACE score and see the difference in predictive validity.

```{r}
#load the RSQLite library
library(RSQLite)
library(here)
library(dplyr)
library(tidyverse)
library(broom)
library(caret)
library(ggplot2)
library(plotly)
library(ROCR)
library(skimr)
library(randomForest)
library(e1071)
library(GGally)
```

```{r}

#connect to our database
con <- dbConnect(drv=SQLite(), dbname=here("data/patient1.sqlite"))

dbListTables(con)
```

```{r}
#Combine points to form LACE score
sqlStatement <- "SELECT patientid, Length_of_stay, Admission_acuity, ifnull(coPts,0) comorbidity, CASE WHEN ifnull(coPts,0)<3 THEN ifnull(coPts,0) ELSE 5 END coPts, ED_visits_score, (Length_of_stay+Admission_acuity+CASE WHEN ifnull(coPts,0)<3 THEN ifnull(coPts,0) ELSE 5 END+ED_visits_score) LACE, readmit30 FROM (
                  (SELECT * 
                  FROM analyticTable 
                  WHERE index_admit=1) LEFT JOIN 
                  (SELECT patientID, SUM(DxPoints) coPts
                  FROM comorbidityScoreTable
                  GROUP BY patientid) USING (patientID)
                )"


queryResult <- tbl(con, sql(sqlStatement))

queryResult

tableResult <-collect(queryResult)

saveRDS(tableResult, file='data/LACE.RDS')
```

# EDA

```{r}
LACE_pt <- readRDS("data/LACE.RDS")

ggplot(gather(LACE_pt), aes(x = value)) +
  geom_histogram() + 
  facet_wrap(~key, scales = 'free_x')

skim(LACE_pt)
summary(LACE_pt)
```

No missing data, 21904 observations of 8 variables.

Factors in this dataset are:
Admission_acuity, either 0 or 3
readmit30, either 0 or 1
ED_visits_score, 1, 2, 3, or 4
coPts, 0, 1, 2, or 5

and comorbidity can be argued either way, as either numeric or factor. Let's treat it as factor moving forward.

# Data Wrangling

```{r}

# Retype the factors

LACE_pt$readmit30 <-factor(LACE_pt$readmit30,labels=c('no','yes'))
LACE_pt$Admission_acuity <- factor(LACE_pt$Admission_acuity)
LACE_pt$ED_visits_score <- factor(LACE_pt$ED_visits_score)
LACE_pt$coPts <- factor(LACE_pt$coPts)
LACE_pt$comorbidity <- factor(LACE_pt$comorbidity)

# Summarize data

summary(LACE_pt)
skim(LACE_pt)

# Look at every pairing

no.PtID <- LACE_pt[2:8]
ggpairs(no.PtID)

```

At a glance, nothing in the ggpairs indicates that any one variable is specifically predictive of readmission, or even that there's high correlation between independent variables (L, coPts, and Admission_acuity). The full LACE score also doesn't indicate it will be terribly predictive on its own, suggesting that we might be missing something huge here.

```{r}
qplot(x = LACE_pt$patientID, y = LACE_pt$LACE, col=LACE_pt$readmit30, shape = LACE_pt$Admission_acuity)

qplot(x = LACE_pt$patientID, y = LACE_pt$Length_of_stay, col = LACE_pt$readmit30)

qplot(x = LACE_pt$patientID, y = LACE_pt$Admission_acuity, col = LACE_pt$readmit30)

qplot(x = LACE_pt$patientID, y = LACE_pt$coPts, col = LACE_pt$readmit30)

qplot(x = LACE_pt$patientID, y = LACE_pt$ED_visits_score, col = LACE_pt$readmit30)
```


I wanna test things after scaling the Length_of_stay because there is an outlier that looks like it could be pulling the data.


```{r}
LACE_pt.scaled <- LACE_pt %>%
  mutate(Length_of_stay = scale(Length_of_stay, center = TRUE))
```


# Create Training and Testing Datasets

Setting type for each of the clearly factor variables ensure they're not treated as numeric in the model.

TECHNICALLY, we should also have a validation set, but in the interest of time and ease of explanation, will go with suggestions from assignment instructions of splitting into 2 partitions: training and testing.

```{r}
set.seed(111)

#grab indices of the dataset that represent 80% of the data
trainingIndices <- createDataPartition(y = LACE_pt$readmit30, p=.80,
                                       list=FALSE)

#show the first few training indices
trainingIndices[1:10]

#select the rows
trainData <- LACE_pt[trainingIndices,]

#confirm the number of rows (should be 80)
nrow(trainData)

#build our test set using the R-indexing
#using the "-" operator
testData <- LACE_pt[-trainingIndices,]

#confirm the number of rows 
nrow(testData)
```

Secondary set of training/testing data - specifically for scaled.

```{r}
trainData.scaled <- LACE_pt.scaled[trainingIndices,]

testData.scaled <- LACE_pt.scaled[-trainingIndices,]

nrow(testData.scaled)
nrow(trainData.scaled)
```

Confirm partitioned data are as expected.

```{r}
# Expected rows:

.8 * nrow(LACE_pt)

# Confirm:

nrow(LACE_pt) == nrow(testData) + nrow(trainData)
```

```{r}
skim(testData)
skim(trainData)

ggplot(gather(testData[2:8]), aes(x = value)) + 
  geom_histogram(stat = 'count') + 
  facet_wrap(~key, scales = 'free_x')

```

```{r}
ggplot(gather(trainData[2:8]), aes(x = value)) +
  geom_histogram(stat = 'count') + 
  facet_wrap(~key, scales = 'free_x')
```

The distributions of the different elements generally look similar in the data partitions, so it's ok to proceed with modeling.

# Linear Model

```{r}
#show variable names in analytic data.frame
colnames(trainData)

#run a simple logistic regression model just using age and gender
#we can cast gender as categorical data using factor()

LACEModel <- glm(readmit30 ~ Length_of_stay + Admission_acuity + coPts + ED_visits_score, data= trainData, family=binomial(link = 'logit'))
summary(LACEModel)
```


And the scaled version.
```{r}
LACEModel.scaled <- glm(readmit30 ~ Length_of_stay + Admission_acuity + coPts + ED_visits_score, data = trainData.scaled, family = binomial(link = 'logit'))

summary(LACEModel.scaled)
```


```{r}
ggplot(trainData, aes(y = readmit30, x = Length_of_stay, color = Admission_acuity)) + 
  geom_boxplot()

ggplot(trainData, aes(y = readmit30, x = Length_of_stay, color = coPts)) + 
  geom_boxplot()

ggplot(trainData, aes(y = readmit30, x = Length_of_stay, color = ED_visits_score)) + 
  geom_boxplot()

ggplot(trainData, aes(y = readmit30, x = coPts, color = ED_visits_score)) + 
  geom_count()
```




```{r}
#Summarize the model
tidy(LACEModel)
```
```{r}
tidy(LACEModel.scaled)
```


```{r}
#grab coefficients themselves
coef(LACEModel)
```
```{r}
modelPredictedProbabilities <- predict(LACEModel, newdata=testData, type = "response")

##add the modelPredictedProbabilities as a column in testData
testData <- data.frame(testData, predProb=modelPredictedProbabilities)
#testDataAugment <- augment(ageGenderModel)

testData %>% ggplot(aes(x=LACE, y=predProb, color=readmit30)) + geom_point()
```

```{r}
modelPredictedProbabilities.scaled <- predict(LACEModel.scaled, newdata=testData.scaled, type = "response")

##add the modelPredictedProbabilities as a column in testData
testData.scaled <- data.frame(testData.scaled, predProb=modelPredictedProbabilities.scaled)
#testDataAugment <- augment(ageGenderModel)

testData.scaled %>% ggplot(aes(x=LACE, y=predProb, color=readmit30)) + geom_point()
```


```{r}
#predict the logit instead for our testData
modelPredictedLogOddsRatio <- predict(LACEModel,newdata = testData)

#add as another column in our table
testDataPred <- data.frame(testData, predLogit = modelPredictedLogOddsRatio)

#plot the age versus logit (coloring by gender)
testDataPred %>% ggplot(aes(x=LACE, y=predLogit, color=readmit30)) + geom_point()
```

```{r}
#predict the logit instead for our testData
modelPredictedLogOddsRatio.scaled <- predict(LACEModel.scaled,newdata = testData.scaled)

#add as another column in our table
testDataPred.scaled <- data.frame(testData.scaled, predLogit = modelPredictedLogOddsRatio.scaled)

#plot the age versus logit (coloring by gender)
testDataPred.scaled %>% ggplot(aes(x=LACE, y=predLogit, color=readmit30)) + geom_point()
```



```{r}
#transform the logit to the predictedOdds ratio
modelPredictedOddsRatio <- exp(modelPredictedLogOddsRatio)
modelPredictedOddsRatio[1:10]

#add as column in our table
testDataPred <- data.frame(testDataPred, predOR = modelPredictedOddsRatio)

#plot Odds ratio versus age
testDataPred %>% ggplot(aes(x=LACE, y=predOR, col=readmit30)) + geom_point() + xlim(0,25) + ylim(0,0.5)
```

```{r}
exp(coef(LACEModel))
```

```{r}
modelPredictedProbabilities %>% data.frame() %>% ggplot(aes(x=modelPredictedProbabilities)) + geom_histogram()
```

```{r}
modelPredictions <- ifelse(modelPredictedProbabilities < 0.13, "no", "yes")
modelPredictions[1:10]
```

```{r}
modelPredictions.scaled <- ifelse(modelPredictedProbabilities.scaled < 0.13, "no", "yes")
modelPredictions.scaled[1:10]
```


```{r}
truthPredict <- table(testData$readmit30, modelPredictions)
truthPredict
```



```{r}
totalCases <- sum(truthPredict)
misclassified <- truthPredict[1,2] + truthPredict[2,1]
misclassified
accuracy <- (totalCases - misclassified) / totalCases
accuracy
```


```{r}

caret::confusionMatrix(testData$readmit30, factor(modelPredictions), positive = "yes")

confusionMatrix(testData.scaled$readmit30, factor(modelPredictions.scaled), positive = "yes")

```


The scaled model does not appear to perform better than the non-scaled model.

```{r}
truth <- factor(testData$readmit30, levels=c("yes","no"))
#seq() function allows us to make a sequence of cutoffs
cutoffs <- seq(from=0.01,to=1, by = 0.01)
cutoffs
```

```{r}
# this function will produce a confusion matrix
# for a particular cutoff
run_cutoff <- function(cutoff){
  #Do thresholding with cutoff       
  predictions <- ifelse(modelPredictedProbabilities < cutoff, "no", "yes")
  #run confusion matrix on predictions
  caret::confusionMatrix(
          factor(predictions, levels=c("yes", "no")), 
                  truth, 
                  positive="yes")
  }
```

```{r}
#run our function on all of our cutoffs
#the output is a list of confusion matrices
confusion_matrices <- lapply(cutoffs, run_cutoff)

#get the sensitivity out of the list of confusion matrices
sensitivities <- lapply(confusion_matrices, function(x){return(x$byClass["Sensitivity"])})

#get the sensitivity out of the list of confusion matrices
specificities <- lapply(confusion_matrices, function(x){return(x$byClass["Specificity"])})

#make sensitivities and specificities into vectors
sensitivities <- unlist(sensitivities)
specificities <- unlist(specificities)

#produce a data frame with all of the cutoffs
cutoff_frame <- data.frame(cutoff=cutoffs, sensitivity =sensitivities, specificity = specificities)
cutoff_frame
```

50% accuracy with a simple linear model is .. not great. Adjusting the components didn't help at all. Cutoff needs to be 0.15 to have at least a random chance at specificity, and keep sensitivity above random chance; 0.11 has best sensitivity, but very poor specificity.

Simplifying the linear model to use only a couple of variables (the most significant found above):

```{r}
LACEModelSimple <- glm(readmit30 ~ Length_of_stay + Admission_acuity, data = trainData, family = "binomial")

summary(LACEModelSimple)

tidy(LACEModelSimple)
```

```{r}
#predict the SIMPLE logit instead for our testData
simplemodelPredictedLogOddsRatio <- predict(LACEModelSimple,newdata = testData)

#add as another column in our table
testDataPredsimple <- data.frame(testData, predLogit = simplemodelPredictedLogOddsRatio)

#plot the age versus logit (coloring by gender)
testDataPredsimple %>% ggplot(aes(x=LACE, y=predLogit, color=readmit30)) + geom_point()
```

```{r}
simplemodelPredictedProbabilities <- predict(LACEModelSimple, newdata=testData, type = "response")

testData <- data.frame(testData, predProbSimple=simplemodelPredictedProbabilities)

testData %>%
  ggplot(aes(x = LACE, y = predProbSimple, color = readmit30)) +
  geom_point()
```

```{r}
simplemodelPredictions <- ifelse(simplemodelPredictedProbabilities < 0.13, "no", "yes")

simpletruthPredict <- table(testData$readmit30, simplemodelPredictions)
simpletruthPredict

caret::confusionMatrix(testData$readmit30, factor(simplemodelPredictions), positive = "yes")
```


Simplified performs no better.


# Random Forest

I want a dataset that only has the data we're going to use for the models in it. 

```{r}
workingtrainData <- LACE_pt[trainingIndices,] %>%
  select(2, 3, 5, 6, 8)

workingtestData <- LACE_pt[-trainingIndices,] %>%
  select(2, 3, 5, 6, 8)
```


```{r}
set.seed(1701)

rf.LACE <- randomForest(readmit30 ~ ., data = workingtrainData, mtry = 4, importance = TRUE, ntree = 10)

rf.LACE

```
```{r}
summary(rf.LACE)
```
```{r}
workingtestData <- mutate(workingtestData, rf.predictLACE = predict(rf.LACE, workingtestData, type = 'response'))

rf.table <- table(workingtestData$readmit30, workingtestData$rf.predictLACE)

rf.table

```

```{r}

rf.misclassified <- rf.table[1,2] + rf.table[2,1]
rf.misclassified
rf.accuracy <- (nrow(workingtestData) - rf.misclassified) / nrow(workingtestData)
rf.accuracy
```


```{r}
rf.conf <- confusionMatrix(workingtestData$readmit30, workingtestData$rf.predictLACE, positive = 'yes')

rf.conf
```


Ok, sensitivity is still pretty ... awful. Maybe we should try allowing the forest to use less than all of the LACE components? Tested using 500 trees (forest), 50 trees (grove), and 10 trees (yard). Highest sensitivity gained was 0.46 with 0.85 for specificity.

```{r}
workingtestData2 <- workingtestData[1:5]
```

```{r}
set.seed(1702)

rfsmall.LACE <- randomForest(readmit30 ~ ., data = workingtrainData, mtry = 2, importance = TRUE, ntree = 10)

```

```{r}
workingtestData2 <- mutate(workingtestData2, rfsmall.predictLACE = predict(rfsmall.LACE, workingtestData2, type = 'response'))

rfsmall.table <- table(workingtestData2$readmit30, workingtestData2$rfsmall.predictLACE)

rfsmall.table
```


```{r}
rfsmall.conf <- confusionMatrix(workingtestData2$readmit30, workingtestData2$rfsmall.predictLACE, positive = 'yes')

rfsmall.conf
```

Sensitivity, arguably more important than specificity in this case, is still pretty low, but much better than when we expected the decision trees to use all the parameters to classify. I'm starting to wonder if we need to reeavaluate our bin values for the ED visits, but that shouldn't capture *more* of the readmit patients, and specificity doesn't need any help with either model.

Trying a SVM because, why not?

# SVM

Because por que no?

```{r}
svm.workingtestData <- workingtestData[1:5]
```

```{r}
set.seed(1703)

tune.out <- tune(svm, readmit30 ~ ., data = workingtrainData, kernel = 'linear', cost = 10)
```

```{r}
summary(tune.out)
```

```{r}
svm.workingtestData <- svm.workingtestData %>%
  mutate(svm.pred = predict(tune.out$best.model, svm.workingtestData))
```

```{r}
svm.conf <- confusionMatrix(svm.workingtestData$readmit30, svm.workingtestData$svm.pred, positive = 'yes')

svm.conf
```

SVM will not identify any positive classes. Adjusted C parameter in range (0.01, .1, 1, 5, 10, 100) - took FOREVER to run, and didn't improve the model at all. SVM is a bust for analyzing this dataset.

# Neural Network

Barrett ran a Neural Network with similar results to the SVM; it is not identifying positive readmit cases.

```{r}
dbDisconnect(con)
```