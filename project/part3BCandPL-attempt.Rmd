---
title: 'Problem Set 3: Modeling'
author: "Campbell/Langasek"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Due Date: August 31 (Monday of Week 10)

1) Calculate the LACE scores for each patient. 

2 Run a basic analysis of the LACE variables to predict readmission risk using your analytic file. Some kinds of analysis you can do: contingency tables ([more info here](http://www.statmethods.net/stats/frequencies.html) ) or logistic regression (More info [here](https://www.r-bloggers.com/evaluating-logistic-regression-models/) and [here](https://www.analyticsvidhya.com/blog/2015/11/beginners-guide-on-logistic-regression-in-r/) ). How well does the LACE score predict readmission risk? Can you compare the predictive power of each variable (L, A, C, E) to predict risk? Begin to think about what visualizations and analysis supports your conclusions.

3) *Optional*: Try changing the ‘weights’ of each of the individual scores to calculate a different LACE score and see the difference in predictive validity.

```{r}
#load the RSQLite library
library(RSQLite)
# general data manipulation, error calculation, and visualization
library(here)
library(dplyr)
library(tidyverse)
library(GGally)
library(skimr)
library(broom)
library(caret)
library(ggplot2)
library(plotly)
library(ROCR)
# Random Forest
library(randomForest)
# SVM
library(e1071)
# Neural Networks
library(keras)
library(tensorflow)
library(reticulate)
```

# Load Data

```{r}

#connect to our database
con <- dbConnect(drv=SQLite(), dbname=here("data/patient1.sqlite"))

dbListTables(con)
```

```{r}
#Combine points to form LACE score
sqlStatement <- "SELECT patientid, Length_of_stay, Admission_acuity, ifnull(coPts,0) comorbidity, CASE WHEN ifnull(coPts,0)<3 THEN ifnull(coPts,0) ELSE 5 END coPts, ED_visits_score, (Length_of_stay+Admission_acuity+CASE WHEN ifnull(coPts,0)<3 THEN ifnull(coPts,0) ELSE 5 END+ED_visits_score) LACE, readmit30 FROM (
                  (SELECT * 
                  FROM analyticTable 
                  WHERE index_admit=1) LEFT JOIN 
                  (SELECT patientID, SUM(DxPoints) coPts
                  FROM comorbidityScoreTable
                  GROUP BY patientid) USING (patientID)
                )"


queryResult <- tbl(con, sql(sqlStatement))

queryResult

tableResult <-collect(queryResult)

saveRDS(tableResult, file='data/LACE.RDS')

LACE_pt <- readRDS("data/LACE.RDS") # Full data file

```

# EDA

```{r}

ggplot(gather(LACE_pt), aes(x = value)) +
  geom_histogram() + 
  facet_wrap(~key, scales = 'free_x')

skim(LACE_pt)
summary(LACE_pt)
```

No missing data, 21904 observations of 8 variables.

Factors in this dataset are:
Admission_acuity, either 0 or 3
readmit30, either 0 or 1
ED_visits_score, 1, 2, 3, or 4
coPts, 0, 1, 2, or 5

and comorbidity can be argued either way, as either numeric or factor. Let's treat it as factor moving forward.

# Data Wrangling

```{r}

# Retype the factors

LACE_pt$readmit30 <-factor(LACE_pt$readmit30,labels=c('no','yes'))
LACE_pt$Admission_acuity <- factor(LACE_pt$Admission_acuity)
LACE_pt$ED_visits_score <- factor(LACE_pt$ED_visits_score)
LACE_pt$coPts <- factor(LACE_pt$coPts)
LACE_pt$comorbidity <- factor(LACE_pt$comorbidity)

# Summarize data

summary(LACE_pt)
skim(LACE_pt)

# Look at every pairing

no.PtID <- LACE_pt[2:8]
ggpairs(no.PtID)

```

At a glance, nothing in the ggpairs indicates that any one variable is specifically predictive of readmission, or even that there's high correlation between independent variables (L, coPts, and Admission_acuity). The full LACE score also doesn't indicate it will be terribly predictive on its own, suggesting that we might be missing something huge here.

```{r}
qplot(x = LACE_pt$patientID, y = LACE_pt$LACE, col=LACE_pt$readmit30, shape = LACE_pt$Admission_acuity)

qplot(x = LACE_pt$patientID, y = LACE_pt$Length_of_stay, col = LACE_pt$readmit30)

qplot(x = LACE_pt$patientID, y = LACE_pt$Admission_acuity, col = LACE_pt$readmit30)

qplot(x = LACE_pt$patientID, y = LACE_pt$coPts, col = LACE_pt$readmit30)

qplot(x = LACE_pt$patientID, y = LACE_pt$ED_visits_score, col = LACE_pt$readmit30)
```

There doesn't appear to be a nice separation of classes in any particular variable according to LACE score or any of the individual components.


I wanna test things after scaling the Length_of_stay because there is an outlier that looks like it could be pulling the data.


```{r}
LACE_pt.scaled <- LACE_pt %>%
  mutate(Length_of_stay = scale(Length_of_stay, center = TRUE))
```


# Create Training and Testing Datasets

Setting type for each of the clearly factor variables ensure they're not treated as numeric in the model.

TECHNICALLY, we should also have a validation set, but in the interest of time and ease of explanation, will go with suggestions from assignment instructions of splitting into 2 partitions: training and testing.

```{r}
set.seed(111)

#grab indices of the dataset that represent 80% of the data
trainingIndices <- createDataPartition(y = LACE_pt$readmit30, p=.80,
                                       list=FALSE)

#show the first few training indices
trainingIndices[1:10]

#select the rows
trainData <- LACE_pt[trainingIndices,]

#confirm the number of rows (should be 80)
nrow(trainData)

#build our test set using the R-indexing
#using the "-" operator
testData <- LACE_pt[-trainingIndices,]

#confirm the number of rows 
nrow(testData)
```

Secondary set of training/testing data - specifically for scaled.

```{r}
trainData.scaled <- LACE_pt.scaled[trainingIndices,]

testData.scaled <- LACE_pt.scaled[-trainingIndices,]

nrow(testData.scaled)
nrow(trainData.scaled)
```

Confirm partitioned data are as expected.

```{r}
# Expected rows:

.8 * nrow(LACE_pt)

# Confirm:

nrow(LACE_pt) == nrow(testData) + nrow(trainData)
```

```{r}
skim(testData)
skim(trainData)

ggplot(gather(testData[2:8]), aes(x = value)) + 
  geom_histogram(stat = 'count') + 
  facet_wrap(~key, scales = 'free_x')

```

```{r}
ggplot(gather(trainData[2:8]), aes(x = value)) +
  geom_histogram(stat = 'count') + 
  facet_wrap(~key, scales = 'free_x')
```

The distributions of the different elements generally look similar in the data partitions, so it's ok to proceed with modeling.

# Linear Model

```{r}
#show variable names in analytic data.frame
colnames(trainData)

#run a logistic regression model using our LACE components

LACEModel <- glm(readmit30 ~ Length_of_stay + Admission_acuity + coPts + ED_visits_score, data= trainData, family=binomial(link = 'logit'))
summary(LACEModel)
```


And the scaled version.
```{r}
LACEModel.scaled <- glm(readmit30 ~ Length_of_stay + Admission_acuity + coPts + ED_visits_score, data = trainData.scaled, family = binomial(link = 'logit'))

summary(LACEModel.scaled)
```

Model results don't appear that much different between "raw" version and scaled version.

```{r}
ggplot(trainData, aes(y = readmit30, x = Length_of_stay, color = Admission_acuity)) + 
  geom_boxplot()

ggplot(trainData, aes(y = readmit30, x = Length_of_stay, color = coPts)) + 
  geom_boxplot()

ggplot(trainData, aes(y = readmit30, x = Length_of_stay, color = ED_visits_score)) + 
  geom_boxplot()

ggplot(trainData, aes(y = readmit30, x = coPts, color = ED_visits_score)) + 
  geom_count()
```

There really is poor deliniation between our readmit classes and the 4 variables.


```{r}
#Summarize the model
tidy(LACEModel)
```
```{r}
tidy(LACEModel.scaled)
```


```{r}
#grab coefficients themselves
coef(LACEModel)
```
```{r}
modelPredictedProbabilities <- predict(LACEModel, newdata=testData, type = "response")

##add the modelPredictedProbabilities as a column in testData
testData <- data.frame(testData, predProb=modelPredictedProbabilities)


testData %>% ggplot(aes(x=LACE, y=predProb, color=readmit30)) + geom_point()
```

Again, no clear class separation.

```{r}
modelPredictedProbabilities.scaled <- predict(LACEModel.scaled, newdata=testData.scaled, type = "response")

##add the modelPredictedProbabilities as a column in testData
testData.scaled <- data.frame(testData.scaled, predProb=modelPredictedProbabilities.scaled)


testData.scaled %>% ggplot(aes(x=LACE, y=predProb, color=readmit30)) + geom_point()
```


```{r}
#predict the logit instead for our testData
modelPredictedLogOddsRatio <- predict(LACEModel,newdata = testData)

#add as another column in our table
testDataPred <- data.frame(testData, predLogit = modelPredictedLogOddsRatio)

#plot the age versus logit (coloring by gender)
testDataPred %>% ggplot(aes(x=LACE, y=predLogit, color=readmit30)) + geom_point()
```

```{r}
#predict the logit instead for our testData
modelPredictedLogOddsRatio.scaled <- predict(LACEModel.scaled,newdata = testData.scaled)

#add as another column in our table
testDataPred.scaled <- data.frame(testData.scaled, predLogit = modelPredictedLogOddsRatio.scaled)

#plot the age versus logit (coloring by gender)
testDataPred.scaled %>% ggplot(aes(x=LACE, y=predLogit, color=readmit30)) + geom_point()
```



```{r}
#transform the logit to the predictedOdds ratio
modelPredictedOddsRatio <- exp(modelPredictedLogOddsRatio)
modelPredictedOddsRatio[1:10]

#add as column in our table
testDataPred <- data.frame(testDataPred, predOR = modelPredictedOddsRatio)


testDataPred %>% ggplot(aes(x=LACE, y=predOR, col=readmit30)) + geom_point() + xlim(0,25) + ylim(0,0.5)
```

```{r}
exp(coef(LACEModel))
```

```{r}
modelPredictedProbabilities %>% data.frame() %>% ggplot(aes(x=modelPredictedProbabilities)) + geom_histogram()
```

```{r}
modelPredictions <- ifelse(modelPredictedProbabilities < 0.13, "no", "yes")
modelPredictions[1:10]
```

```{r}
modelPredictions.scaled <- ifelse(modelPredictedProbabilities.scaled < 0.13, "no", "yes")
modelPredictions.scaled[1:10]
```


```{r}
truthPredict <- table(testData$readmit30, modelPredictions)
truthPredict
```



```{r}
totalCases <- sum(truthPredict)
misclassified <- truthPredict[1,2] + truthPredict[2,1]
misclassified
accuracy <- (totalCases - misclassified) / totalCases
accuracy
```


```{r}

caret::confusionMatrix(testData$readmit30, factor(modelPredictions), positive = "yes")

confusionMatrix(testData.scaled$readmit30, factor(modelPredictions.scaled), positive = "yes")

```


The scaled model does not appear to perform better than the non-scaled model. Sensitivity is very low, while specificity is through the roof; I suspect this is because of the overrepresentation of negative outcomes and lack of clear class distinction.

```{r}
truth <- factor(testData$readmit30, levels=c("yes","no"))
#seq() function allows us to make a sequence of cutoffs
cutoffs <- seq(from=0.01,to=1, by = 0.01)
cutoffs
```

```{r}
# this function will produce a confusion matrix
# for a particular cutoff
run_cutoff <- function(cutoff){
  #Do thresholding with cutoff       
  predictions <- ifelse(modelPredictedProbabilities < cutoff, "no", "yes")
  #run confusion matrix on predictions
  caret::confusionMatrix(
          factor(predictions, levels=c("yes", "no")), 
                  truth, 
                  positive="yes")
  }
```

```{r}
#run our function on all of our cutoffs
#the output is a list of confusion matrices
confusion_matrices <- lapply(cutoffs, run_cutoff)

#get the sensitivity out of the list of confusion matrices
sensitivities <- lapply(confusion_matrices, function(x){return(x$byClass["Sensitivity"])})

#get the sensitivity out of the list of confusion matrices
specificities <- lapply(confusion_matrices, function(x){return(x$byClass["Specificity"])})

#make sensitivities and specificities into vectors
sensitivities <- unlist(sensitivities)
specificities <- unlist(specificities)

#produce a data frame with all of the cutoffs
cutoff_frame <- data.frame(cutoff=cutoffs, sensitivity =sensitivities, specificity = specificities)
cutoff_frame
```

50% accuracy with a simple linear model is .. not great. Adjusting the components didn't help at all. Cutoff needs to be 0.15 to have at least a random chance at specificity, and keep sensitivity above random chance; 0.11 has best sensitivity, but very poor specificity.

Simplifying the linear model to use only a couple of variables (the most significant found above):

```{r}
LACEModelSimple <- glm(readmit30 ~ Length_of_stay + Admission_acuity, data = trainData, family = "binomial")

summary(LACEModelSimple)

tidy(LACEModelSimple)
```

```{r}
#predict the SIMPLE logit instead for our testData
simplemodelPredictedLogOddsRatio <- predict(LACEModelSimple,newdata = testData)

#add as another column in our table
testDataPredsimple <- data.frame(testData, predLogit = simplemodelPredictedLogOddsRatio)

#plot the age versus logit (coloring by gender)
testDataPredsimple %>% ggplot(aes(x=LACE, y=predLogit, color=readmit30)) + geom_point()
```

```{r}
simplemodelPredictedProbabilities <- predict(LACEModelSimple, newdata=testData, type = "response")

testData <- data.frame(testData, predProbSimple=simplemodelPredictedProbabilities)

testData %>%
  ggplot(aes(x = LACE, y = predProbSimple, color = readmit30)) +
  geom_point()
```

```{r}
simplemodelPredictions <- ifelse(simplemodelPredictedProbabilities < 0.13, "no", "yes")

simpletruthPredict <- table(testData$readmit30, simplemodelPredictions)
simpletruthPredict

caret::confusionMatrix(testData$readmit30, factor(simplemodelPredictions), positive = "yes")
```


Simplified performs no better.


# Random Forest

I want a dataset that only has the data we're going to use for the models in it. 

```{r}
workingtrainData <- LACE_pt[trainingIndices,] %>%
  select(2, 3, 5, 6, 8)

workingtestData <- LACE_pt[-trainingIndices,] %>%
  select(2, 3, 5, 6, 8)
```


```{r}
set.seed(1701)

rf.LACE <- randomForest(readmit30 ~ ., data = workingtrainData, mtry = 4, importance = TRUE, ntree = 10)

rf.LACE

```
```{r}
summary(rf.LACE)
```
```{r}
workingtestData <- mutate(workingtestData, rf.predictLACE = predict(rf.LACE, workingtestData, type = 'response'))

rf.table <- table(workingtestData$readmit30, workingtestData$rf.predictLACE)

rf.table

```

```{r}

rf.misclassified <- rf.table[1,2] + rf.table[2,1]
rf.misclassified
rf.accuracy <- (nrow(workingtestData) - rf.misclassified) / nrow(workingtestData)
rf.accuracy
```


```{r}
rf.conf <- confusionMatrix(workingtestData$readmit30, workingtestData$rf.predictLACE, positive = 'yes')

rf.conf
```


Ok, sensitivity is still pretty ... awful. Maybe we should try allowing the forest to use less than all of the LACE components? Tested using 500 trees (forest), 50 trees (grove), and 10 trees (yard). Highest sensitivity gained was 0.46 with 0.85 for specificity. Certainly performing better than logistic regression.

```{r}
workingtestData2 <- workingtestData[1:5]
```

```{r}
set.seed(1702)

rfsmall.LACE <- randomForest(readmit30 ~ ., data = workingtrainData, mtry = 2, importance = TRUE, ntree = 10)

```

```{r}
workingtestData2 <- mutate(workingtestData2, rfsmall.predictLACE = predict(rfsmall.LACE, workingtestData2, type = 'response'))

rfsmall.table <- table(workingtestData2$readmit30, workingtestData2$rfsmall.predictLACE)

rfsmall.table
```


```{r}
rfsmall.conf <- confusionMatrix(workingtestData2$readmit30, workingtestData2$rfsmall.predictLACE, positive = 'yes')

rfsmall.conf
```

Sensitivity, arguably more important than specificity in this case, is still pretty low, but much better than when we expected the decision trees to use all the parameters to classify. I'm starting to wonder if we need to reeavaluate our bin values for the ED visits, but that shouldn't capture *more* of the readmit patients, and specificity doesn't need any help with either model.

Trying a SVM because, why not?

# SVM

Because por que no?

```{r}
svm.workingtestData <- workingtestData[1:5]
```

```{r}
set.seed(1703)

tune.out <- tune(svm, readmit30 ~ ., data = workingtrainData, kernel = 'linear', cost = 10)
```

```{r}
summary(tune.out)
```

```{r}
svm.workingtestData <- svm.workingtestData %>%
  mutate(svm.pred = predict(tune.out$best.model, svm.workingtestData))
```

```{r}
svm.conf <- confusionMatrix(svm.workingtestData$readmit30, svm.workingtestData$svm.pred, positive = 'yes')

svm.conf
```

SVM will not identify any positive classes. Adjusted C parameter in range (0.01, .1, 1, 5, 10, 100) - took FOREVER to run, and didn't improve the model at all. SVM is a bust for analyzing this dataset.

# Neural Network

Barrett ran a Neural Network with similar results to the SVM; it also does not perform well in this dataset.

```{r}
## some Intel CPUs need this option set for Keras to work
## if in doubt, just leave this command uncommented
Sys.setenv('KMP_DUPLICATE_LIB_OK'='True')

## first let's remember what our data looks like
head(trainData)

## normalize data
scaled_train <- apply(data.matrix(trainData), 2, function(x) (x-min(x))/(max(x) - min(x)))
scaled_test <- apply(data.matrix(testData), 2, function(x) (x-min(x))/(max(x) - min(x)))

## our dataset still contains all of the same information, but values are now numeric and range from 0 to 1
head(scaled_train)
```

```{r}
## instantiate the model framework and let keras know we'll be building a sequential model
model <- keras_model_sequential()

## add fully-connected/dense layers to our model with dropout for regularization (i.e. to prevent overfitting)
## scaled_train[,c(2,6) selects all rows from columns 2 and 6 (age and sbp) of the training data
model %>%
	layer_dense(units = 140, activation = 'relu', input_shape = ncol(scaled_test[,c("Length_of_stay","Admission_acuity", "coPts", "ED_visits_score")])) %>%
	layer_dropout(0.25) %>%
	layer_dense(units = 140, activation = 'relu') %>%
	layer_dropout(0.25) %>%
	layer_dense(units = 1, activation = 'sigmoid')

## compile the model
## since this is a binary classification problem, we use binary crossentropy as our loss metric
## we'll use simple stochastic gradient decent as our optimizer
## and we'll monitor the accuracy of both the train and test sets to let us evaluate model performance
model %>% compile(loss = "binary_crossentropy", optimizer = optimizer_sgd(lr = 0.1), metrics = c('acc'))
```

```{r, results='hide'}
## select our predictor and outcome variables 
scaled_train_x <- scaled_train[, c('Length_of_stay','Admission_acuity','coPts','ED_visits_score')]
scaled_train_y <- scaled_train[, 'readmit30']

## train the model
training <- model %>% fit(x = scaled_train_x, y = scaled_train_y, validation_split = 0.20, batch_size = 100, epochs = 25)
plot(training)
```

```{r}
## as before, we have our model make predictions on the test dataset
nn_predictions <- model %>% predict_proba(scaled_test[,c("Length_of_stay","Admission_acuity", "coPts", "ED_visits_score")])

## compare those predictions to the true class labels
nn_pr <- prediction(nn_predictions, scaled_test[,"readmit30"])

## evaluate the performance of the model
nn_perf <- performance(nn_pr, measure = "tpr", x.measure = "fpr")

## calculate the AUC of the model
nn_auc <- performance(nn_pr, measure = "auc")
nn_auc <- nn_auc@y.values[[1]]

## plot the model performance
plot(nn_perf, main="ROC")
abline(a=0, b=1, col='red', lty=2)
text(0.5, 0.15, paste('AUROC: ', round(nn_auc, 3)))

## we can also have the neural network output the predicted class (rather than class probability)
nn_classes <- model %>% predict_classes(scaled_test[,c("Length_of_stay","Admission_acuity", "coPts", "ED_visits_score")])

## then use that as input to confusionMatrix()
nn_conf <- confusionMatrix(as.factor(nn_classes), as.factor(scaled_test[,"readmit30"]), positive = '1')
nn_conf
```

Sensitivity is still very very low. Again, I suspect it's the low number of readmission cases in our data.


```{r}
dbDisconnect(con)
```